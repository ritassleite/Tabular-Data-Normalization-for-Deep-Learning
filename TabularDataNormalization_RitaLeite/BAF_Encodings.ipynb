{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "Hfwzsu8Hn_fh",
    "outputId": "20fbcbd9-4eb0-44b7-c021-de830713c31c"
   },
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import scipy\n",
    "import opendatasets as od\n",
    "import math\n",
    "import torchmetrics\n",
    "import importlib\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders\n",
    "import pickle\n",
    "import io\n",
    "import yaml              \n",
    "\n",
    "from matplotlib import pyplot as plt            \n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from sklearn import metrics                     \n",
    "\n",
    "import utils.mlp as mlp\n",
    "import utils.mlp_pipeline as mlp_pipeline\n",
    "import utils.embedding_pipeline as embedding_pipeine\n",
    "\n",
    "from bank_account_fraud.notebooks.random_search import RandomValueTrial, suggest_callable_hyperparams  # from repository https://github.com/feedzai/bank-account-fraud.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcC1B0Uk6lp5"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNKh5QAYoAHz"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M24dfxIUoFYG",
    "outputId": "826884a3-d264-498e-a1cb-6034e7aac151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELUgMJ7toIcM",
    "outputId": "366bb555-01e4-4d1f-dc76-c3496906dc0a"
   },
   "outputs": [],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022?select=Base.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIgmNosYoLPH"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "extension = \"csv\"  # or \"parquet\", depending on the downloaded file\n",
    "data_paths = glob.glob(f\"</path/to/datasets/>*.{extension}\")\n",
    "\n",
    "def read_dataset(path, ext=extension):\n",
    "    if ext == \"csv\":\n",
    "        return pd.read_csv(path, index_col=0)\n",
    "    elif ext == \"parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid extension: '{ext}'.\")\n",
    "\n",
    "def get_variant(path):\n",
    "        return path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "dataframes = {\n",
    "    get_variant(path): read_dataset(path) for path in data_paths\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0etk8ti2oPDS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read hyperparameter space for the LGBM Models, expected structure is presented bellow\n",
    "with open(\"/content/gdrive/MyDrive/tab_norm_folder/bank_account_fraud/notebooks/lightgbm_hyperparameter_space.yaml\", \"r\") as file:\n",
    "    hyperparam_space = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "# Define path to datasets. Replace `base_path` with the appropriate value.\n",
    "base_path = \"/content/bank-account-fraud-dataset-neurips-2022/\"\n",
    "\n",
    "datasets_paths = {\n",
    "    \"Base\":    base_path + \"Base.csv\",\n",
    "}\n",
    "\n",
    "datasets = {key: pd.read_csv(path) for key, path in datasets_paths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3h-ddffoglg",
    "outputId": "9c726768-371e-4ee7-e00d-88030b233ad1"
   },
   "outputs": [],
   "source": [
    "print(datasets['Base'].describe())\n",
    "\n",
    "\n",
    "# Create the train and test sets. Shuffle data with `sample` method.\n",
    "# The split was done by month. The first 6 months as the train, the last 2 months as test.\n",
    "train_dfs = {key: df[df[\"month\"]<6].sample(frac=1, replace=False) for key, df in datasets.items()}\n",
    "val_dfs = {key: df[df[\"month\"]==6].sample(frac=1, replace=False) for key, df in datasets.items()}\n",
    "test_dfs= {key: df[df[\"month\"]==7].sample(frac=1, replace=False) for key, df in datasets.items()}\n",
    "\n",
    "label = \"fraud_bool\"\n",
    "\n",
    "categorical_features = [\n",
    "    \"payment_type\",\n",
    "    \"employment_status\",\n",
    "    \"housing_status\",\n",
    "    \"source\",\n",
    "    \"device_os\",\n",
    "]\n",
    "\n",
    "\n",
    "for name in datasets.keys():  # For each dataset in the suite\n",
    "    train = train_dfs[name]\n",
    "    val=val_dfs[name]\n",
    "    test = test_dfs[name]\n",
    "    for feat in categorical_features:\n",
    "      encoder = LabelEncoder()\n",
    "      encoder.fit(train[feat])  # Fit an encoder to the train set.\n",
    "      train[feat] = encoder.transform(train[feat])  # Transform train set.\n",
    "      val[feat] = encoder.transform(val[feat])  # Transform val set.\n",
    "      test[feat] = encoder.transform(test[feat])    # Transform test set.\n",
    "\n",
    "for dataset_name in datasets.keys():  # Run hyperparameters on all variants of datastes.\n",
    "        \n",
    "      X_train = train_dfs[dataset_name].drop(columns=[\"fraud_bool\"])\n",
    "      y_train = train_dfs[dataset_name][\"fraud_bool\"]\n",
    "      X_val = val_dfs[dataset_name].drop(columns=[\"fraud_bool\"])\n",
    "      y_val = val_dfs[dataset_name][\"fraud_bool\"]\n",
    "      X_test=test_dfs[dataset_name].drop(columns=[\"fraud_bool\"])\n",
    "      y_test = test_dfs[dataset_name][\"fraud_bool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g27uDywwoz9j"
   },
   "outputs": [],
   "source": [
    "#Establish continuous and categorical features\n",
    "cat_cols=categorical_features\n",
    "cont_cols=train.columns.difference(cat_cols)\n",
    "cont_cols=cont_cols.difference(['fraud_bool'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Zblq9zL6vWk"
   },
   "source": [
    "# Embeddings\n",
    "Defining Embeddings for Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ap1yLKvMo05T",
    "outputId": "030f5d0a-101f-4e8d-8abc-4487a3f00d81"
   },
   "outputs": [],
   "source": [
    "\n",
    "for name in datasets.keys():  # For each dataset in the suite\n",
    "    train = train_dfs[name]\n",
    "    val=val_dfs[name]\n",
    "    test = test_dfs[name]\n",
    "\n",
    "embedding_pipeline=importlib.reload(embedding_pipeline)\n",
    "x_train,x_test,x_val=embedding_pipeline.aggregate_low_card_BAF(X_train,X_test,X_val,cat_cols)\n",
    "dims=embedding_pipeline.get_emb_dim(x_train,'log',cat_cols)\n",
    "\n",
    "dims=torch.tensor(dims,dtype=int).to(device)\n",
    "\n",
    "import mlp\n",
    "import mlp_pipeline\n",
    "\n",
    "param_list=mlp.mlp_param_sampler(20, len(train.columns),7,device)\n",
    "\n",
    "\n",
    "x_train_cont=x_train[cont_cols]\n",
    "x_train_cat=x_train[cat_cols]\n",
    "x_test_cont=x_test[cont_cols]\n",
    "x_test_cat=x_test[cat_cols]\n",
    "x_val_cont=x_val[cont_cols]\n",
    "x_val_cat=x_val[cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjVuBRfeq5Gi"
   },
   "outputs": [],
   "source": [
    "normalization='Zscore'\n",
    "xtrain_aux_cont=mlp_pipeline.normalization_transform(x_train_cont,normalization,cont_cols)\n",
    "xval_aux_cont=mlp_pipeline.normalization_transform(x_val_cont,normalization,cont_cols)\n",
    "\n",
    "xtrain_aux_cat=mlp_pipeline.normalization_transform(x_train_cat,'None',[])\n",
    "xval_aux_cat=mlp_pipeline.normalization_transform(x_val_cat,'None',[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhBIYW-jpFVU"
   },
   "outputs": [],
   "source": [
    "\n",
    "normalization='Zscore'\n",
    "xtrain_aux_cont=mlp_pipeline.normalization_transform(x_train_cont,normalization,cont_cols)\n",
    "xval_aux_cont=mlp_pipeline.normalization_transform(x_val_cont,normalization,cont_cols)\n",
    "\n",
    "xtrain_aux_cat=mlp_pipeline.normalization_transform(x_train_cat,'None',[])\n",
    "xval_aux_cat=mlp_pipeline.normalization_transform(x_val_cat,'None',[])\n",
    "method='log'\n",
    "\n",
    "dims=embedding_pipeline.get_emb_dim(x_train,method,cat_cols)\n",
    "\n",
    "dims=torch.tensor(dims,dtype=int).to(device)\n",
    "\n",
    "i=0\n",
    "\n",
    "for params in param_list[i:]:\n",
    "\n",
    "        # Fit pipeline\n",
    "        print('iteration: ',i)\n",
    "        \n",
    "        model=embedding_pipeline.pipeline(device,xtrain_aux_cat,xtrain_aux_cont,xval_aux_cat,xval_aux_cont,y_train,y_val,params,'log',dims)\n",
    "\n",
    "        #Save the model\n",
    "        joblib.dump(model,'tab_norm_folder/embedding/mlp_simple{}{}{}.pkl'.format(method,normalization,i))\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ai9vexj8f-He"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_pipeline=importlib.reload(embedding_pipeline)\n",
    "\n",
    "normalization='Zscore'\n",
    "xtrain_aux_cont=mlp_pipeline.normalization_transform(x_train_cont,normalization,cont_cols)\n",
    "xval_aux_cont=mlp_pipeline.normalization_transform(x_val_cont,normalization,cont_cols)\n",
    "\n",
    "xtrain_aux_cat=mlp_pipeline.normalization_transform(x_train_cat,'None',[])\n",
    "xval_aux_cat=mlp_pipeline.normalization_transform(x_val_cat,'None',[])\n",
    "method='sqrt'\n",
    "\n",
    "dims=embedding_pipeline.get_emb_dim(x_train,method,cat_cols)\n",
    "\n",
    "dims=torch.tensor(dims,dtype=int).to(device)\n",
    "\n",
    "i=0\n",
    "\n",
    "for params in param_list[i:]:\n",
    "\n",
    "        # Fit pipeline\n",
    "        print('iteration: ',i)\n",
    "        \n",
    "        model=embedding_pipeline.pipeline(device,xtrain_aux_cat,xtrain_aux_cont,xval_aux_cat,xval_aux_cont,y_train,y_val,params,'log',dims)\n",
    "\n",
    "        #Save the model\n",
    "        joblib.dump(model,'tab_norm_folder/embedding/mlp_simple{}{}{}.pkl'.format(method,normalization,i))\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIISeaUQDro7"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_pipeline=importlib.reload(embedding_pipeline)\n",
    "i=0\n",
    "for normalization in ['MinMax']:\n",
    "  xtrain_aux_cont=mlp_pipeline.normalization_transform(x_train_cont,normalization,cont_cols)\n",
    "  xval_aux_cont=mlp_pipeline.normalization_transform(x_val_cont,normalization,cont_cols)\n",
    "\n",
    "  xtrain_aux_cat=mlp_pipeline.normalization_transform(x_train_cat,'None',[])\n",
    "  xval_aux_cat=mlp_pipeline.normalization_transform(x_val_cat,'None',[])\n",
    "  method='sqrt'\n",
    "\n",
    "  dims=embedding_pipeline.get_emb_dim(x_train,method,cat_cols)\n",
    "\n",
    "  dims=torch.tensor(dims,dtype=int)\n",
    "\n",
    "\n",
    "  for params in param_list[i:]:\n",
    "\n",
    "        # Fit pipeline\n",
    "        print('iteration: ',i)\n",
    "        \n",
    "        model=embedding_pipeline.pipeline(device,xtrain_aux_cat,xtrain_aux_cont,xval_aux_cat,xval_aux_cont,y_train,y_val,params,'log',dims)\n",
    "\n",
    "        #Save the model\n",
    "        joblib.dump(model,'tab_norm_folder/embedding/mlp_simple{}{}{}.pkl'.format(method,normalization,i))\n",
    "\n",
    "        i=i+1\n",
    "  i=0\n",
    "  del xtrain_aux_cat\n",
    "  del xtrain_aux_cont\n",
    "  del xval_aux_cat\n",
    "  del xval_aux_cont\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85zKhcec2rzM",
    "outputId": "b4a3f328-528b-4537-b06a-957f86c5dabe"
   },
   "outputs": [],
   "source": [
    "\n",
    "embedding_pipeline=importlib.reload(embedding_pipeline)\n",
    "mlp_params=mlp.mlp_param_sampler(20, len(train.columns),7,device)\n",
    "\n",
    "\n",
    "xtrain_aux_cat=mlp_pipeline.normalization_transform(x_train_cat,'None',[])\n",
    "xval_aux_cat=mlp_pipeline.normalization_transform(x_val_cat,'None',[])\n",
    "method='sqrt'\n",
    "\n",
    "dims=embedding_pipeline.get_emb_dim(x_train,method,cat_cols)\n",
    "\n",
    "dims=torch.tensor(dims,dtype=int).to(device)\n",
    "encoder=data.PiecewiseLinearEncoder('decision_tree',dict(n_bins=10,regression='False', tree_kwargs={'min_samples_leaf': 128}),stack=False)\n",
    "encoder.fit(torch.tensor(X_train[cont_cols.difference(['device_fraud_count'])].values),torch.tensor(y_train.values)) \n",
    "#the device count fraud feature is constant, and so cannot be fed to this encoder\n",
    "x_train_cont=encoder.transform(torch.tensor(X_train[cont_cols.difference(['device_fraud_count'])].values))\n",
    "x_val_cont=encoder.transform(torch.tensor(X_val[cont_cols.difference(['device_fraud_count'])].values))\n",
    "i=0\n",
    "del encoder\n",
    "gc.collect()\n",
    "for param_list in mlp_params[i:]:        \n",
    "\n",
    "        \n",
    "        # Fit pipeline\n",
    "        print('iteration: ',i)\n",
    "        \n",
    "        model=embedding_pipeline.pipeline(device,xtrain_aux_cat,x_train_cont,xval_aux_cat,x_val_cont,y_train,y_val,param_list,'sqrt',dims)\n",
    "\n",
    "        #Save the model\n",
    "        joblib.dump(model,'tab_norm_folder/embedding/mlp_num_enc10{}{}.pkl'.format(method,i))\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKhhpKoPEtlN",
    "outputId": "7ca354f9-e090-4904-d542-1ea1ad5206f4"
   },
   "outputs": [],
   "source": [
    "#Attempting embeddings with dimension 1\n",
    "\n",
    "embedding_pipeline=importlib.reload(embedding_pipeline)\n",
    "i=10\n",
    "for normalization in ['Median']:\n",
    "  xtrain_aux_cont=mlp_pipeline.normalization_transform(x_train_cont,normalization,cont_cols)\n",
    "  xval_aux_cont=mlp_pipeline.normalization_transform(x_val_cont,normalization,cont_cols)\n",
    "\n",
    "  xtrain_aux_cat=mlp_pipeline.normalization_transform(x_train_cat,'None',[])\n",
    "  xval_aux_cat=mlp_pipeline.normalization_transform(x_val_cat,'None',[])\n",
    "\n",
    "\n",
    "  dims=embedding_pipeline.get_emb_dim(x_train,'ones',cat_cols)\n",
    "\n",
    "  dims=torch.tensor(dims,dtype=int)\n",
    "\n",
    "\n",
    "  for params in param_list[i:]:\n",
    "\n",
    "        # Fit pipeline\n",
    "        print('iteration: ',i)\n",
    "        \n",
    "        model=embedding_pipeline.pipeline(device,xtrain_aux_cat,xtrain_aux_cont,xval_aux_cat,xval_aux_cont,y_train,y_val,params,'dim=1',dims)\n",
    "\n",
    "        #Save the model\n",
    "        joblib.dump(model,'tab_norm_folder/embedding/mlp_simple{}{}{}.pkl'.format('1dimensional',normalization,i))\n",
    "\n",
    "        i=i+1\n",
    "  i=0\n",
    "  del xtrain_aux_cat\n",
    "  del xtrain_aux_cont\n",
    "  del xval_aux_cat\n",
    "  del xval_aux_cont\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAiMEH3I8pql"
   },
   "source": [
    "# Numerical Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNrQ8mCIlD5w"
   },
   "outputs": [],
   "source": [
    "#Code for the numerical embeddings from : https://github.com/Yura52/rtdl\n",
    "import sys\n",
    "sys.path.append(\"content/gdrive/MyDrive\")\n",
    "\n",
    "from rtdl.rtdl import data as data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "JpeyKlq08WtL",
    "outputId": "230c4867-80d6-4da7-b033-468f5892b608"
   },
   "outputs": [],
   "source": [
    "link=\"tab_norm_folder/target_encoderBAF.pkl\"\n",
    "file = open(link,'rb')\n",
    "encoder = joblib.load(link)\n",
    "x_train_cat=encoder.transform(X_train)[cat_cols]\n",
    "x_val_cat=encoder.transform(X_val)[cat_cols]\n",
    "encoder=data.PiecewiseLinearEncoder('decision_tree',dict(n_bins=10,regression='False', tree_kwargs={'min_samples_leaf': 128}),stack=False)\n",
    "\n",
    "#Fitting the encoder: some binary features are also encoded, this line will raise warning due to the amount of distinct values. \n",
    "encoder.fit(torch.tensor(X_train[cont_cols.difference(['device_fraud_count'])].values),torch.tensor(y_train.values)) #the device count fraud feature is constant, and so cannot be fed to this encoder\n",
    "x_train_cont=encoder.transform(torch.tensor(X_train[cont_cols.difference(['device_fraud_count'])].values))\n",
    "x_val_cont=encoder.transform(torch.tensor(X_val[cont_cols.difference(['device_fraud_count'])].values))  \n",
    "x_train_aux=torch.cat([torch.tensor(x_train_cont),torch.tensor(x_train_cat.values)],1)\n",
    "x_val_aux=torch.cat([torch.tensor(x_val_cont),torch.tensor(x_val_cat.values)],1)\n",
    "\n",
    "joblib.dump(encoder,\"tab_norm_folder/encoders/numerical_encoder_dim10.pkl\")\n",
    "\n",
    "del encoder\n",
    "gc.collect()\n",
    "\n",
    "mlp_params=mlp.mlp_param_sampler(20, len(train.columns),7,device)\n",
    "i=15\n",
    "for param_list in mlp_params[i:]:\n",
    "    print(\" Iteration:\",i)\n",
    "\n",
    "    model=mlp_pipeline.pipeline(device,x_train_aux,x_val_aux,y_train,y_val,param_list,'None',cont_cols.difference(['device_fraud_count']))\n",
    "    joblib.dump(model,\"tab_norm_folder/numerical_embeddings/num_enc_mlp_target{}_dim.pkl\".format(i) )\n",
    "    i=i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link=\"tab_norm_folder/cat_encoderBAF.pkl\"\n",
    "file = open(link,'rb')\n",
    "encoder = joblib.load(link)\n",
    "x_train_cat=encoder.transform(X_train)[cat_cols]\n",
    "x_val_cat=encoder.transform(X_val)[cat_cols]\n",
    "encoder=data.PiecewiseLinearEncoder('decision_tree',dict(n_bins=10,regression='False', tree_kwargs={'min_samples_leaf': 128}),stack=False)\n",
    "\n",
    "#Fitting the encoder: some binary features are also encoded, this line will raise warning due to the amount of distinct values. \n",
    "encoder.fit(torch.tensor(X_train[cont_cols.difference(['device_fraud_count'])].values),torch.tensor(y_train.values)) #the device count fraud feature is constant, and so cannot be fed to this encoder\n",
    "x_train_cont=encoder.transform(torch.tensor(X_train[cont_cols.difference(['device_fraud_count'])].values))\n",
    "x_val_cont=encoder.transform(torch.tensor(X_val[cont_cols.difference(['device_fraud_count'])].values))  \n",
    "x_train_aux=torch.cat([torch.tensor(x_train_cont),torch.tensor(x_train_cat.values)],1)\n",
    "x_val_aux=torch.cat([torch.tensor(x_val_cont),torch.tensor(x_val_cat.values)],1)\n",
    "\n",
    "joblib.dump(encoder,\"tab_norm_folder/encoders/numerical_encoder_dim10.pkl\")\n",
    "\n",
    "del encoder\n",
    "gc.collect()\n",
    "\n",
    "mlp_params=mlp.mlp_param_sampler(20, len(train.columns),7,device)\n",
    "i=0\n",
    "for param_list in mlp_params[i:]:\n",
    "    print(\" Iteration:\",i)\n",
    "\n",
    "    model=mlp_pipeline.pipeline(device,x_train_aux,x_val_aux,y_train,y_val,param_list,'None',cont_cols.difference(['device_fraud_count']))\n",
    "    joblib.dump(model,\"tab_norm_folder/numerical_embeddings/num_enc_mlp_cat{}_dim.pkl\".format(i) )\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMtpzEhy0LV3",
    "outputId": "4ffeb2cf-ca8f-4236-cd28-724b7b10538c"
   },
   "outputs": [],
   "source": [
    "link=\"tab_norm_folder/count_encoderBAF.pkl\"\n",
    "file = open(link,'rb')\n",
    "encoder = joblib.load(link)\n",
    "x_train_cat=encoder.transform(X_train)[cat_cols]\n",
    "x_val_cat=encoder.transform(X_val)[cat_cols]\n",
    "encoder=data.PiecewiseLinearEncoder('decision_tree',dict(n_bins=10,regression='False', tree_kwargs={'min_samples_leaf': 128}),stack=False)\n",
    "\n",
    "#Fitting the encoder: some binary features are also encoded, this line will raise warning due to the amount of distinct values. \n",
    "encoder.fit(torch.tensor(X_train[cont_cols.difference(['device_fraud_count'])].values),torch.tensor(y_train.values)) #the device count fraud feature is constant, and so cannot be fed to this encoder\n",
    "x_train_cont=encoder.transform(torch.tensor(X_train[cont_cols.difference(['device_fraud_count'])].values))\n",
    "x_val_cont=encoder.transform(torch.tensor(X_val[cont_cols.difference(['device_fraud_count'])].values))  \n",
    "x_train_aux=torch.cat([torch.tensor(x_train_cont),torch.tensor(x_train_cat.values)],1)\n",
    "x_val_aux=torch.cat([torch.tensor(x_val_cont),torch.tensor(x_val_cat.values)],1)\n",
    "\n",
    "joblib.dump(encoder,\"tab_norm_folder/encoders/numerical_encoder_dim10.pkl\")\n",
    "\n",
    "del encoder\n",
    "gc.collect()\n",
    "\n",
    "mlp_params=mlp.mlp_param_sampler(20, len(train.columns),7,device)\n",
    "i=0\n",
    "for param_list in mlp_params[i:]:\n",
    "    print(\" Iteration:\",i)\n",
    "\n",
    "    model=mlp_pipeline.pipeline(device,x_train_aux,x_val_aux,y_train,y_val,param_list,'None',cont_cols.difference(['device_fraud_count']))\n",
    "    joblib.dump(model,\"/content/gdrive/MyDrive/tab_norm_folder/numerical_embeddings/num_enc_mlp_count{}_dim.pkl\".format(i) )\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ3fAv6G4c6h"
   },
   "source": [
    "# TABTRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783,
     "referenced_widgets": [
      "fc8aee9a0a054fa692fb7dce9dc5aff7",
      "c4608d6651e94a109d1bbb679cf8cb33"
     ]
    },
    "id": "n6bpGzuC4hGe",
    "outputId": "36f41767-c7a1-4ce6-d72a-f3a746798e18"
   },
   "outputs": [],
   "source": [
    "import utils.tabtransformer as tabtransformer\n",
    "\n",
    "cat_cols=categorical_features\n",
    "cont_cols=['bank_branch_count_8w', 'bank_months_count', 'credit_risk_score',\n",
    "       'current_address_months_count', 'customer_age',\n",
    "       'date_of_birth_distinct_emails_4w', 'days_since_request',\n",
    "       'device_distinct_emails_8w', 'device_fraud_count', 'email_is_free',\n",
    "       'foreign_request', 'has_other_cards', 'income',\n",
    "       'intended_balcon_amount', 'keep_alive_session', 'month',\n",
    "       'name_email_similarity', 'phone_home_valid', 'phone_mobile_valid',\n",
    "       'prev_address_months_count', 'proposed_credit_limit',\n",
    "       'session_length_in_minutes', 'velocity_24h', 'velocity_4w',\n",
    "       'velocity_6h', 'zip_count_4w']\n",
    "\n",
    "params=tabtransformer.tabtransformer_param_sampler(20,7,device)\n",
    "X_train_norm=mlp_pipeline.z_score(X_train,cont_cols)\n",
    "X_val_norm=mlp_pipeline.z_score(X_val,cont_cols)\n",
    "i=0\n",
    "for param in params[i:]:\n",
    "  model=tabtransformer.transformer_pipeline(param,cat_cols,cont_cols,['fraud_bool'],X_train_norm,X_val_norm,y_train,y_val,'cpu')\n",
    "  joblib.dump(model,\"tab_norm_folder/tabtransformer/tabtransformer_zscore{}.pkl\".format(i))\n",
    "  i=i+1\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ct9H92VR82E"
   },
   "outputs": [],
   "source": [
    "X_val_norm=mlp_pipeline.z_score(X_val,cont_cols)\n",
    "X_test_norm=mlp_pipeline.z_score(X_test,cont_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "pYh1y9QDcAnK",
    "outputId": "49c81609-5c95-4702-f967-66309ef9f0c7"
   },
   "outputs": [],
   "source": [
    "for i in range(0,8):\n",
    "  model=CPU_Unpickler(open('tabtransformer/tabtransformer_zscore{}.pkl'.format(i),'rb')).load()\n",
    "  predict=model.predict(X_val_norm)['1_probability']\n",
    "  joblib.dump(predict,'res_prediction/yval{}.pkl'.format(i))\n",
    "  predict=model.predict(X_test_norm)['1_probability']\n",
    "  joblib.dump(predict,'res_prediction/yhat{}.pkl'.format(i))\n",
    "  break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aAiMEH3I8pql"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "c4608d6651e94a109d1bbb679cf8cb33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc8aee9a0a054fa692fb7dce9dc5aff7": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_c4608d6651e94a109d1bbb679cf8cb33",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 5/29 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3529/3529</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:02:57 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">23.02it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">loss: 0.283 train_loss: 0.237    </span>\n                                                                                  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">valid_loss: 2.309 valid_accuracy:</span>\n                                                                                  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.495 train_accuracy: 0.88       </span>\n</pre>\n",
         "text/plain": "Epoch 5/29 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m3529/3529\u001b[0m \u001b[38;5;245m0:02:57 • 0:00:00\u001b[0m \u001b[38;5;249m23.02it/s\u001b[0m \u001b[37mloss: 0.283 train_loss: 0.237    \u001b[0m\n                                                                                  \u001b[37mvalid_loss: 2.309 valid_accuracy:\u001b[0m\n                                                                                  \u001b[37m0.495 train_accuracy: 0.88       \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
